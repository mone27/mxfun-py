{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "async file download in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from tqdm.auto import tqdm\n",
    "# import request\n",
    "from pathlib import Path\n",
    "from humanize import naturalsize\n",
    "import asyncio\n",
    "from fastcore.utils import *\n",
    "\n",
    "import aiohttp\n",
    "import aiofiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def add_suffix(path: Path, suffix): return path.parent / (path.name + suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "async def download_file(url,\n",
    "                        out_name,\n",
    "                        session,\n",
    "                        semaphore,\n",
    "                        master_bar=None,\n",
    "                        chunk_size = 1024,\n",
    "                        overwrite = False,\n",
    "                        wait = 10,\n",
    "                        retry = 0,\n",
    "                        max_redirects = 1,\n",
    "                        max_retry = 1,\n",
    "                        max_wait = 10 * 60\n",
    "                       ):\n",
    "    if out_name.exists() and not overwrite:\n",
    "        print(f\"Skipping: {url}\")\n",
    "        if master_bar is not None: master_bar.update()\n",
    "        return\n",
    "    try:\n",
    "        # limit number of concurret requests\n",
    "        async with semaphore:\n",
    "            async with session.get(url, max_redirects = max_redirects) as resp:\n",
    "                tot_size = resp.headers['Content-Length']\n",
    "                with tqdm(total = int(tot_size), unit='B', unit_scale=True, unit_divisor=1024) as pbar:\n",
    "                    print(f\"Downloading: {url}\", naturalsize(tot_size))\n",
    "                    async with aiofiles.open(out_name.add_suffix(\".part\"), 'wb') as fd:\n",
    "                        async for chunk in resp.content.iter_chunked(chunk_size):\n",
    "                            await fd.write(chunk)\n",
    "                            pbar.set_postfix(file=out_name.name[-10:], retry = retry, refresh=False)\n",
    "                            pbar.update(chunk_size)\n",
    "    except (aiohttp.TooManyRedirects, aiohttp.ClientPayloadError) as e:\n",
    "        if retry > max_retry:\n",
    "            raise ValueError(f\"Too many retries, {retry}\")\n",
    "        print(f\"Server error {out_name.name}. Retrying in {wait} s [{retry}/{max_retry}]\")\n",
    "        await asyncio.sleep(wait)\n",
    "        try:\n",
    "            pbar.clear()\n",
    "        except NameError:\n",
    "            pass # no progress bar created so no need to clear it\n",
    "        # try again\n",
    "        return await download_file(url, out_name,\n",
    "                            session, semaphore, master_bar,\n",
    "                            chunk_size, overwrite,\n",
    "                            wait = wait + 60 if wait < max_wait else wait,\n",
    "                            retry = retry + 1,\n",
    "                            max_retry = max_retry,\n",
    "                            max_redirects = max_redirects\n",
    "                                  )\n",
    "        \n",
    "    try:\n",
    "        if overwrite:\n",
    "            out_name.unlink(missing_ok = True)\n",
    "        out_name.add_suffix(\".part\").rename(out_name)\n",
    "        print(f\"Done: {out_name.stem}, {naturalsize(out_name.stat().st_size)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed: {out_name.stem}\")\n",
    "        print(e)\n",
    "    if master_bar is not None: master_bar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "async def download_files(urls, out_names, overwrite = False, max_redirects = 5, max_retry = 10, timeout = 5 * 60, max_connections = 100):\n",
    "    sem = asyncio.Semaphore(max_connections)\n",
    "    with tqdm(total = len(urls)) as master_bar:\n",
    "         async with aiohttp.ClientSession(timeout = aiohttp.ClientTimeout(timeout)) as session:\n",
    "            await asyncio.gather(*[download_file(\n",
    "                url, out, session, sem, master_bar = master_bar,\n",
    "                overwrite = overwrite, max_redirects = max_redirects, max_retry = max_retry)\n",
    "                                   for url, out in zip(urls, out_names)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely import box\n",
    "from pyogrio import read_info, read_bounds\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from dask.distributed import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def file_bbox(f: Path):\n",
    "    \"\"\"get (fast) bounding box of all features in vector file\"\"\"\n",
    "    info = read_info(f)\n",
    "    if info['features'] > 0:\n",
    "        _, b = read_bounds(f)\n",
    "        bound_geom = box(\n",
    "                np.nanmin(b[0, :]),  # minx\n",
    "                np.nanmin(b[1, :]),  # miny\n",
    "                np.nanmax(b[2, :]),  # maxx\n",
    "                np.nanmax(b[3, :]),  # maxy\n",
    "        )\n",
    "        return gpd.GeoDataFrame({'geometry': [bound_geom], 'path': f, 'file_name': f.name}, crs = info['crs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def write_bbox_tileindex_dir(\n",
    "    dir_path: Path, # path of folder with individual files\n",
    "    out_name: Path, # name of the \n",
    "    starts_with:str = \"\", # optional filter files names\n",
    "    file_type:str = \".shp\", # file extension\n",
    "    relative:bool=True, # output path relative to\n",
    "    dask_client:Client = None # optionally parallilize on dask\n",
    ") -> gpd.GeoDataFrame: # tileindex\n",
    "    \"\"\"make and save a tileindex with bounding boxes of all files in a directory. Optionally run on dask\"\"\"\n",
    "    files = dir_path.ls().filter(lambda f: f.suffix == file_type and f.name.startswith(starts_with))\n",
    "    if dask_client is not None: bounds = dask_client.gather(dask_client.map(file_bbox, files))\n",
    "    else: bounds = files.progress_map(file_bbox)\n",
    "    tileindex = pd.concat(bounds)\n",
    "    tileindex = tileindex.assign(path = tileindex.path.apply(lambda p: str(p.relative_to(out_name.parent))) if relative else tileindex.path.apply(str))\n",
    "    tileindex.to_file(out_name)\n",
    "    return tileindex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L extension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from tqdm.auto import tqdm\n",
    "from fastcore.basics import patch, map_ex\n",
    "from fastcore.foundation import L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def progress_map(self: L, f, *args, **kwargs): return self._new(map_ex(tqdm(self), f, *args, gen=False, **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "813f1b96e11b4c1ba36e087570afa179",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(#5) [None,None,None,None,None]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L([1]*5).progress_map(time.sleep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GeoPandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def progress_apply(self: gpd.geodataframe.GeoDataFrame|gpd.geoseries.GeoSeries, func, **kwargs):\n",
    "        total = len(self)\n",
    "        t = tqdm(total=total)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            t.update(n=1 if not t.total or t.n < t.total else 0)\n",
    "            return func(*args, **kwargs)\n",
    "        try:\n",
    "            return self.apply(wrapper, **kwargs)\n",
    "        finally:\n",
    "            t.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def iter_row_df(df): return [df.iloc[i:i+1] for i in range(len(df))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def pipe(self: L, f, *args, **kwargs): return f(self, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## path utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from pyprojroot import here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def str_here(x): return str(here(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
